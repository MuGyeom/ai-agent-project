import time
import trafilatura
from duckduckgo_search import DDGS
from common.config import settings
from common.utils import KafkaConsumerWrapper, KafkaProducerWrapper
from common.database import SessionLocal, Request, SearchResult


def search_and_crawl(topic, max_results=8):
    """
    1. DuckDuckGo ê²€ìƒ‰
    2. ìƒìœ„ Nê°œ URL ìˆ˜ì§‘
    3. ë³¸ë¬¸ í¬ë¡¤ë§ (ê°œì„ ëœ trafilatura ì„¤ì •)
    4. ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (DB ì €ì¥ìš©)
    """
    print(f"ğŸ” Searching for: {topic}")
    results = []

    # 1. ê²€ìƒ‰ ìˆ˜í–‰
    try:
        with DDGS() as ddgs:
            search_results = list(ddgs.text(topic, max_results=max_results))

        for result in search_results:
            url = result["href"]
            title = result["title"]
            print(f"   ğŸ‘‰ Found: {title} ({url})")

            # 2. ë³¸ë¬¸ í¬ë¡¤ë§ (trafilatura ê³ ê¸‰ ì„¤ì •)
            try:
                downloaded = trafilatura.fetch_url(url)
                content = ""
                
                if downloaded:
                    # ê°œì„ ëœ ì¶”ì¶œ ì„¤ì •
                    text = trafilatura.extract(
                        downloaded,
                        include_comments=False,      # ëŒ“ê¸€ ì œì™¸
                        include_tables=True,          # í‘œ í¬í•¨
                        no_fallback=False,            # fallback í—ˆìš© (ë” ë§ì€ ì½˜í…ì¸ )
                        favor_precision=False,        # recall ìš°ì„  (ë” ë§ì€ í…ìŠ¤íŠ¸)
                        favor_recall=True,
                        deduplicate=True,             # ì¤‘ë³µ ì œê±°
                        target_language="ko",         # í•œêµ­ì–´ ìš°ì„ 
                    )
                    
                    if text and len(text.strip()) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                        # ë” ê¸´ ë³¸ë¬¸ í—ˆìš© (8000ìê¹Œì§€)
                        content = text.strip()[:8000]
                        print(f"      âœ… Extracted {len(content)} characters")
                    else:
                        print(f"      âš ï¸ Content too short ({len(text) if text else 0} chars)")
                else:
                    print(f"      âš ï¸ Failed to fetch {url}")
                    
            except Exception as e:
                print(f"      âŒ Crawl error for {url}: {e}")
                content = ""

            # ê²°ê³¼ ì €ì¥ (ë¹ˆ ë‚´ìš©ì´ë¼ë„ ì €ì¥ - ì œëª©/URLì€ ìœ ìš©)
            results.append({
                "url": url,
                "title": title,
                "content": content
            })

            time.sleep(1)  # ì°¨ë‹¨ ë°©ì§€

    except Exception as e:
        print(f"âŒ Search Error: {e}")

    # ìœ íš¨í•œ ì½˜í…ì¸ ê°€ ìˆëŠ” ê²°ê³¼ë§Œ ë°˜í™˜
    valid_results = [r for r in results if r["content"]]
    print(f"ğŸ“Š Total: {len(results)} results, Valid: {len(valid_results)} with content")
    
    return valid_results if valid_results else results[:3]  # ìµœì†Œ 3ê°œëŠ” ë°˜í™˜


def process_search():
    consumer = KafkaConsumerWrapper(
        topic=settings.KAFKA_TOPIC_SEARCH, group_id=settings.KAFKA_GROUP_SEARCH
    )
    producer = KafkaProducerWrapper()

    print(f"ğŸš€ [Search Worker] Ready using DuckDuckGo...")

    for message in consumer.get_messages():
        db = SessionLocal()
        try:
            task = message.value
            request_id = task.get("request_id")
            topic = task.get("topic")

            print(f"\n{'='*60}")
            print(f"Received request: {request_id} : {topic}")

            # ğŸ”’ Pessimistic Lock: Row-level locking
            # SELECT FOR UPDATE SKIP LOCKED prevents race conditions
            from sqlalchemy import text
            
            # Try to acquire exclusive lock on this request
            lock_query = text("""
                SELECT id, status 
                FROM requests 
                WHERE id = :request_id 
                AND status = 'searching'
                FOR UPDATE SKIP LOCKED
            """)
            
            result = db.execute(lock_query, {"request_id": request_id}).fetchone()
            
            if not result:
                # Either already locked by another worker, or status != 'searching'
                existing = db.query(Request).filter(Request.id == request_id).first()
                if existing:
                    if existing.status == 'searching':
                        print(f"ğŸ”’ Request {request_id} locked by another worker, skipping")
                    else:
                        print(f"â­ï¸  Request {request_id} already processed (status: {existing.status})")
                else:
                    print(f"âŒ Request {request_id} not found")
                consumer.consumer.commit()
                continue

            # We successfully acquired the lock! Update status immediately
            db_request = db.query(Request).filter(Request.id == request_id).first()
            db_request.status = 'processing_search'
            db.commit()
            print(f"âœ… Locked and claimed request {request_id}")

            # ê²€ìƒ‰ ìˆ˜í–‰
            search_results_data = search_and_crawl(topic, max_results=8)

            if not search_results_data:
                print(f"âš ï¸  No search results for {topic}")
                # ìƒíƒœë¥¼ failedë¡œ ì—…ë°ì´íŠ¸
                db_request.status = "failed"
                db_request.error_message = "No search results found"
                db.commit()
                consumer.consumer.commit()
                continue

            # DBì— ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            for result_data in search_results_data:
                search_result = SearchResult(
                    request_id=request_id,
                    url=result_data['url'],
                    title=result_data['title'],
                    content=result_data['content']
                )
                db.add(search_result)
            
            db.commit()
            print(f"ğŸ’¾ Saved {len(search_results_data)} search results to DB")

            # ìš”ì²­ ìƒíƒœ ì—…ë°ì´íŠ¸: processing_search â†’ analyzing
            db_request.status = "analyzing"
            db.commit()

            # AI Workerì— ë¶„ì„ ìš”ì²­ ì „ë‹¬
            producer.send_data(
                topic=settings.KAFKA_TOPIC_AI,
                value={
                    "request_id": request_id,
                    "topic": topic
                }
            )
            
            # Kafka offset ì»¤ë°‹
            consumer.consumer.commit()
            print(f"âœ… Request {request_id} handed off to AI worker")

        except Exception as e:
            print(f"âŒ Worker Error: {e}")
            # ì—ëŸ¬ ìƒíƒœ ì €ì¥
            if 'request_id' in locals() and request_id:
                db_request = db.query(Request).filter(Request.id == request_id).first()
                if db_request:
                    db_request.status = "failed"
                    db_request.error_message = str(e)
                    db.commit()
        finally:
            db.close()


if __name__ == "__main__":
    process_search()
