import os
from vllm import LLM, SamplingParams
from common.config import settings
from common.utils import KafkaConsumerWrapper

# vLLM ëª¨ë¸ ì´ˆê¸°í™” (Global - í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
print("ğŸ”§ Initializing vLLM Engine...")
MODEL_NAME = os.getenv("VLLM_MODEL", "Qwen/Qwen2.5-7B-Instruct")
GPU_MEMORY_UTIL = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.85"))
MAX_MODEL_LEN = int(os.getenv("VLLM_MAX_MODEL_LEN", "4096"))

try:
    llm = LLM(
        model=MODEL_NAME,
        gpu_memory_utilization=GPU_MEMORY_UTIL,
        max_model_len=MAX_MODEL_LEN,
        trust_remote_code=True,  # Qwen ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”
    )
    print(f"âœ… vLLM Model Loaded: {MODEL_NAME}")
    print(f"   GPU Memory Utilization: {GPU_MEMORY_UTIL * 100}%")
    print(f"   Max Model Length: {MAX_MODEL_LEN} tokens")
except Exception as e:
    print(f"âŒ Failed to load vLLM model: {e}")
    print("ğŸ’¡ Tip: Check GPU availability and CUDA installation")
    raise


def process_ai():
    consumer = KafkaConsumerWrapper(
        topic=settings.KAFKA_TOPIC_AI, group_id=settings.KAFKA_GROUP_AI
    )

    print(f"ğŸ¤– [AI Worker] Ready for inference...")

    # vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=1024,
    )

    for message in consumer.get_messages():
        try:
            task = message.value
            original_topic = task.get("original_topic")
            context = task.get("context")

            print("\n" + "=" * 60)
            print(f"ğŸ“¥ Topic: {original_topic}")
            print(f"ğŸ“„ Context Length: {len(context)} characters")
            print("=" * 60)

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""You are a research assistant. Analyze the following web search results and provide a comprehensive summary.

Topic: {original_topic}

Search Results:
{context}

Please provide:
1. A concise summary of the key findings
2. Main themes and insights
3. Relevant conclusions

Summary:"""

            # vLLM ì¶”ë¡  ì‹¤í–‰
            print("ğŸ§  Generating summary with vLLM...")
            outputs = llm.generate([prompt], sampling_params)
            summary = outputs[0].outputs[0].text.strip()

            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "ğŸ¯ " + "=" * 58)
            print("GENERATED SUMMARY:")
            print("=" * 60)
            print(summary)
            print("=" * 60)
            print(f"âœ… Generated {len(summary)} characters\n")

            # TODO: ì¶”í›„ DB ì €ì¥ ë˜ëŠ” Kafka í† í”½ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
            # producer.send_data(topic="results-queue", value={
            #     "topic": original_topic,
            #     "summary": summary,
            #     "timestamp": time.time()
            # })

        except Exception as e:
            print(f"âŒ Error during inference: {e}")
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    process_ai()
