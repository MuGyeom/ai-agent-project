import os

# CRITICAL: v0 API ê°•ì œ ì‚¬ìš© - vLLM import ì „ì— í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜!
# vLLMì€ import ì‹œì ì— v0/v1ì„ ê²°ì •í•˜ë¯€ë¡œ ë°˜ë“œì‹œ import ì „ì— ì„¤ì •í•´ì•¼ í•¨
os.environ["VLLM_USE_V1"] = "0"
print("ğŸ”’ Forced VLLM_USE_V1=0 (before vLLM import)")

from vllm import LLM, SamplingParams
from common.config import settings
from common.utils import KafkaConsumerWrapper

# vLLM ëª¨ë¸ ì´ˆê¸°í™” (Global - í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
print("ğŸ”§ Initializing vLLM Engine...")
MODEL_NAME = os.getenv("VLLM_MODEL", "Qwen/Qwen2.5-14B-Instruct-AWQ")
GPU_MEMORY_UTIL = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.90"))
MAX_MODEL_LEN = int(os.getenv("VLLM_MAX_MODEL_LEN", "4096"))
QUANTIZATION = os.getenv("VLLM_QUANTIZATION", "awq")  # AWQ 4-bit ì–‘ìí™”

# í™˜ê²½ë³€ìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©)
print(f"ğŸ” Environment Check:")
print(f"   VLLM_USE_V1={os.getenv('VLLM_USE_V1')}")
print(f"   Model: {MODEL_NAME}")

try:
    llm = LLM(
        model=MODEL_NAME,
        quantization=QUANTIZATION,  # AWQ ì–‘ìí™” í™œì„±í™”
        gpu_memory_utilization=GPU_MEMORY_UTIL,
        max_model_len=MAX_MODEL_LEN,
        trust_remote_code=True,  # Qwen ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”
        dtype="half",  # FP16 ì‚¬ìš©
    )
    print(f"âœ… vLLM Model Loaded: {MODEL_NAME}")
    print(f"   Quantization: {QUANTIZATION.upper()}")
    print(f"   GPU Memory Utilization: {GPU_MEMORY_UTIL * 100}%")
    print(f"   Max Model Length: {MAX_MODEL_LEN} tokens")
except Exception as e:
    print(f"âŒ Failed to load vLLM model: {e}")
    print(f"ğŸ’¡ Model: {MODEL_NAME}")
    print(f"ğŸ’¡ Quantization: {QUANTIZATION}")
    print(f"ğŸ’¡ VLLM_USE_V1: {os.getenv('VLLM_USE_V1')}")
    print("ğŸ’¡ Tip: For RTX 4070 (12GB), use AWQ 4-bit quantized models")
    import traceback
    traceback.print_exc()
    raise


def process_ai():
    from common.database import SessionLocal, Request, AnalysisResult
    from datetime import datetime
    import time as time_module
    
    consumer = KafkaConsumerWrapper(
        topic=settings.KAFKA_TOPIC_AI, group_id=settings.KAFKA_GROUP_AI
    )

    print(f"ğŸ¤– [AI Worker] Ready for inference...")

    # vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=1536,
        repetition_penalty=1.1,      # ë°˜ë³µ ë°©ì§€ (1.0 = ì—†ìŒ, 1.1 = ì•½ê°„, 1.5 = ê°•í•¨)
        frequency_penalty=0.2,        # ê°™ì€ í† í° ë°˜ë³µ íŒ¨ë„í‹°
        presence_penalty=0.0,         # ìƒˆë¡œìš´ í† í”½ ìœ ë„
    )

    for message in consumer.get_messages():
        db = SessionLocal()
        start_time = time_module.time()
        try:
            task = message.value
            request_id = task.get("request_id")
            topic = task.get("topic")

            print("\n" + "=" * 60)
            print(f"ğŸ“¥ Request ID: {request_id}")
            print(f"ï¿½ Topic: {topic}")

            # DBì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¡°íšŒ
            db_request = db.query(Request).filter(Request.id == request_id).first()
            if not db_request:
                print(f"âŒ Request {request_id} not found in database")
                continue

            # ğŸ”’ Idempotency: ì´ë¯¸ ì™„ë£Œëœ ìš”ì²­ì€ ìŠ¤í‚µ
            if db_request.status == "completed":
                print(f"â­ï¸ Request {request_id} already completed, skipping duplicate...")
                consumer.consumer.commit()  # Offsetë§Œ ì»¤ë°‹
                continue

            search_results = db_request.search_results
            if not search_results:
                print(f"âŒ No search results found for request {request_id}")
                db_request.status = "failed"
                db_request.error_message = "No search results to analyze"
                db.commit()
                consumer.consumer.commit()  # Offset ì»¤ë°‹
                continue

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            print(f"ğŸ“š Found {len(search_results)} search results")
            context_parts = []
            for idx, result in enumerate(search_results, 1):
                context_parts.append(
                    f"[ê²°ê³¼ {idx}]\n"
                    f"ì œëª©: {result.title}\n"
                    f"URL: {result.url}\n"
                    f"ë‚´ìš©: {result.content}\n"
                )

            context = "\n---\n".join(context_parts)
            print(f"ğŸ“„ Total Context Length: {len(context)} characters")

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (LLM í–‰ë™ ê·œì¹™)
            system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì •ë³´ ìš”ì•½ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
1. **í•œêµ­ì–´ë¡œë§Œ ë‹µë³€**í•˜ì„¸ìš”.
2. **ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ** ë‹µë³€í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. **ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ** í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•˜ì„¸ìš”.
4. **ì¤‘ë³µëœ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”**.
5. ë‹µë³€ì€ **3-5ê°œ ë¬¸ë‹¨** ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
6. ê´€ë ¨ ì—†ëŠ” ê²€ìƒ‰ ê²°ê³¼ëŠ” ë¬´ì‹œí•˜ì„¸ìš”.
7. ì¶œì²˜ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”."""

            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì‹¤ì œ ìš”ì²­)
            user_prompt = f"""ì£¼ì œ: {topic}

ê²€ìƒ‰ ê²°ê³¼:
{context}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{topic}'ì— ëŒ€í•´ ìš”ì•½í•´ì£¼ì„¸ìš”."""

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ (Qwen í˜•ì‹)
            prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""

            # LLM ì¶”ë¡ 
            print("ğŸ§  Analyzing with vLLM...")
            outputs = llm.generate([prompt], sampling_params)
            summary = outputs[0].outputs[0].text.strip()
            
            inference_time_ms = int((time_module.time() - start_time) * 1000)
            
            print(f"âœ… Analysis completed in {inference_time_ms}ms")
            print(f"ğŸ“Š Summary length: {len(summary)} characters")
            print("\n" + "=" * 60)
            print("GENERATED SUMMARY:")
            print("-" * 60)
            print(summary)
            print("=" * 60 + "\n")

            # DBì— ë¶„ì„ ê²°ê³¼ ì €ì¥
            analysis_result = AnalysisResult(
                request_id=request_id,
                summary=summary,
                inference_time_ms=inference_time_ms
            )
            db.add(analysis_result)

            # ìš”ì²­ ìƒíƒœ ì—…ë°ì´íŠ¸: analyzing â†’ completed
            db_request.status = "completed"
            db_request.completed_at = datetime.utcnow()
            db.commit()
            print(f"ğŸ’¾ Analysis result saved to database")

            # âœ… Kafka offset ì»¤ë°‹ (DB ì €ì¥ ì„±ê³µ í›„)
            consumer.consumer.commit()
            print(f"ğŸ“Œ Kafka offset committed")
            print(f"ğŸ‰ Request {request_id} completed!")

        except Exception as e:
            print(f"âŒ AI Worker Error: {e}")
            import traceback
            traceback.print_exc()
            
            # ì—ëŸ¬ ìƒíƒœ ì €ì¥
            if 'request_id' in locals() and request_id:
                db_request = db.query(Request).filter(Request.id == request_id).first()
                if db_request:
                    db_request.status = "failed"
                    db_request.error_message = str(e)
                    db.commit()
        finally:
            db.close()


if __name__ == "__main__":
    process_ai()
