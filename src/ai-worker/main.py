import os

# CRITICAL: v0 API ê°•ì œ ì‚¬ìš© - vLLM import ì „ì— í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜!
# vLLMì€ import ì‹œì ì— v0/v1ì„ ê²°ì •í•˜ë¯€ë¡œ ë°˜ë“œì‹œ import ì „ì— ì„¤ì •í•´ì•¼ í•¨
os.environ["VLLM_USE_V1"] = "0"
print("ğŸ”’ Forced VLLM_USE_V1=0 (before vLLM import)")

from vllm import LLM, SamplingParams
from common.config import settings
from common.utils import KafkaConsumerWrapper

# vLLM ëª¨ë¸ ì´ˆê¸°í™” (Global - í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
print("ğŸ”§ Initializing vLLM Engine...")
MODEL_NAME = os.getenv("VLLM_MODEL", "Qwen/Qwen2.5-14B-Instruct-AWQ")
GPU_MEMORY_UTIL = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.90"))
MAX_MODEL_LEN = int(os.getenv("VLLM_MAX_MODEL_LEN", "4096"))
QUANTIZATION = os.getenv("VLLM_QUANTIZATION", "awq")  # AWQ 4-bit ì–‘ìí™”

# í™˜ê²½ë³€ìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©)
print(f"ğŸ” Environment Check:")
print(f"   VLLM_USE_V1={os.getenv('VLLM_USE_V1')}")
print(f"   Model: {MODEL_NAME}")

try:
    llm = LLM(
        model=MODEL_NAME,
        quantization=QUANTIZATION,  # AWQ ì–‘ìí™” í™œì„±í™”
        gpu_memory_utilization=GPU_MEMORY_UTIL,
        max_model_len=MAX_MODEL_LEN,
        trust_remote_code=True,  # Qwen ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”
        dtype="half",  # FP16 ì‚¬ìš©
    )
    print(f"âœ… vLLM Model Loaded: {MODEL_NAME}")
    print(f"   Quantization: {QUANTIZATION.upper()}")
    print(f"   GPU Memory Utilization: {GPU_MEMORY_UTIL * 100}%")
    print(f"   Max Model Length: {MAX_MODEL_LEN} tokens")
except Exception as e:
    print(f"âŒ Failed to load vLLM model: {e}")
    print(f"ğŸ’¡ Model: {MODEL_NAME}")
    print(f"ğŸ’¡ Quantization: {QUANTIZATION}")
    print(f"ğŸ’¡ VLLM_USE_V1: {os.getenv('VLLM_USE_V1')}")
    print("ğŸ’¡ Tip: For RTX 4070 (12GB), use AWQ 4-bit quantized models")
    import traceback
    traceback.print_exc()
    raise


def process_ai():
    consumer = KafkaConsumerWrapper(
        topic=settings.KAFKA_TOPIC_AI, group_id=settings.KAFKA_GROUP_AI
    )

    print(f"ğŸ¤– [AI Worker] Ready for inference...")

    # vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=1024,
    )

    for message in consumer.get_messages():
        try:
            task = message.value
            original_topic = task.get("original_topic")
            context = task.get("context")

            print("\n" + "=" * 60)
            print(f"ğŸ“¥ Topic: {original_topic}")
            print(f"ğŸ“„ Context Length: {len(context)} characters")
            print("=" * 60)

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""You are a research assistant. Analyze the following web search results and provide a comprehensive summary.

Topic: {original_topic}

Search Results:
{context}

Please provide:
1. A concise summary of the key findings
2. Main themes and insights
3. Relevant conclusions

Summary:"""

            # vLLM ì¶”ë¡  ì‹¤í–‰
            print("ğŸ§  Generating summary with vLLM...")
            outputs = llm.generate([prompt], sampling_params)
            summary = outputs[0].outputs[0].text.strip()

            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "ğŸ¯ " + "=" * 58)
            print("GENERATED SUMMARY:")
            print("=" * 60)
            print(summary)
            print("=" * 60)
            print(f"âœ… Generated {len(summary)} characters\n")

            # TODO: ì¶”í›„ DB ì €ì¥ ë˜ëŠ” Kafka í† í”½ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
            # producer.send_data(topic="results-queue", value={
            #     "topic": original_topic,
            #     "summary": summary,
            #     "timestamp": time.time()
            # })

        except Exception as e:
            print(f"âŒ Error during inference: {e}")
            import traceback

            traceback.print_exc()


if __name__ == "__main__":
    process_ai()
