# 11. Map-Reduce Pattern for Long Context

* Status: Accepted
* Date: 2026-01-29
* Context: Phase 3 (AI Worker Enhancement)

## Context and Problem Statement

ê²€ìƒ‰ ê²°ê³¼ê°€ ë§ê±°ë‚˜ ê° ê²°ê³¼ê°€ ê¸´ ê²½ìš° í† í° ì œí•œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:

1. **í† í° ì´ˆê³¼ ì—ëŸ¬**: `Input prompt (29580 tokens) is too long and exceeds limit of 6144`
2. **ì •ë³´ ì†ì‹¤**: ë‹¨ìˆœ truncationì€ ì¤‘ìš” ì •ë³´ ì†ì‹¤ ê°€ëŠ¥
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±**: ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” GPU ë©”ëª¨ë¦¬ ë¶€ë‹´

**í•„ìš”ì‚¬í•­**:
- í† í° ì œí•œ ë‚´ì—ì„œ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
- ì •ë³´ ì†ì‹¤ ìµœì†Œí™”
- í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜

## Decision Drivers

* **ì •ë³´ ë³´ì¡´**: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í™œìš©
* **í† í° íš¨ìœ¨**: ì œí•œ ë‚´ì—ì„œ ìµœëŒ€ ì •ë³´ ì¶”ì¶œ
* **í’ˆì§ˆ**: ìš”ì•½ í’ˆì§ˆ ìœ ì§€
* **í™•ì¥ì„±**: ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¦ê°€ì—ë„ ëŒ€ì‘

## Considered Options

### Option 1: Simple Truncation
**Pros**: ê°„ë‹¨í•¨
**Cons**: ì •ë³´ ì†ì‹¤

### Option 2: Map-Reduce Pattern âœ…
**Pros**: ëª¨ë“  ì •ë³´ í™œìš©, í™•ì¥ ê°€ëŠ¥
**Cons**: ì¶”ê°€ LLM í˜¸ì¶œ í•„ìš”

### Option 3: Stuffing with Compression
**Pros**: ë‹¨ì¼ í˜¸ì¶œ
**Cons**: ì••ì¶• ì‹œ ì •ë³´ ì†ì‹¤

## Decision Outcome

**Map-Reduce íŒ¨í„´**ì„ ì ìš©í•˜ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

---

## Implementation Details

### Architecture

```
ê²€ìƒ‰ ê²°ê³¼ (Nê°œ)
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  í† í° ìˆ˜ ê³„ì‚°                            â”‚
â”‚  total_tokens = tokenizer.encode(...)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â”€â”€ ì œí•œ ë‚´ â”€â”€â”€â–¶ ì§ì ‘ ë¶„ì„ (Strategy A)
      â”‚
      â””â”€â”€â”€ ì œí•œ ì´ˆê³¼ â”€â”€â–¶ Map-Reduce (Strategy B)
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Chunk 1 â”‚   ...   â”‚ Chunk N â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â”‚                   â”‚
                   â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Summary 1â”‚   ...   â”‚Summary Nâ”‚  â—€â”€â”€ Map Phase
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   â”‚                   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Final Summary  â”‚  â—€â”€â”€ Reduce Phase
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Code

```python
# src/ai_worker/main.py

# 1. Initialize Tokenizer & Calculate Tokens
tokenizer = llm.get_tokenizer()
RESERVED_TOKENS = 1800  # System prompt + output buffer
MAX_CONTEXT_TOKENS = MAX_MODEL_LEN - RESERVED_TOKENS

full_context_str = "\n---\n".join(content_items)
total_tokens = len(tokenizer.encode(full_context_str))

# 2. Strategy Selection
if total_tokens <= MAX_CONTEXT_TOKENS:
    # STRATEGY A: Direct Analysis
    final_context = full_context_str
else:
    # STRATEGY B: Map-Reduce
    MAP_CHUNK_SIZE = 3000  # tokens per chunk
    
    # [Map Phase] Split into chunks and summarize
    chunks = []
    current_chunk, current_tokens = [], 0
    
    for item in content_items:
        item_tokens = len(tokenizer.encode(item))
        if current_tokens + item_tokens > MAP_CHUNK_SIZE:
            chunks.append("\n---\n".join(current_chunk))
            current_chunk, current_tokens = [item], item_tokens
        else:
            current_chunk.append(item)
            current_tokens += item_tokens
    if current_chunk:
        chunks.append("\n---\n".join(current_chunk))
    
    # Batch inference for all chunks
    map_prompts = [build_map_prompt(chunk, topic) for chunk in chunks]
    map_outputs = llm.generate(map_prompts, SamplingParams(temperature=0.7, max_tokens=1024))
    
    intermediate_summaries = [output.outputs[0].text.strip() for output in map_outputs]
    
    # [Reduce Phase] Combine summaries
    final_context = "\n\n---\n\n".join(
        [f"Summary Part {i+1}:\n{s}" for i, s in enumerate(intermediate_summaries)]
    )
```

---

## Performance

| ì‹œë‚˜ë¦¬ì˜¤ | í† í° ìˆ˜ | ì „ëµ | ì¶”ê°€ í˜¸ì¶œ |
|----------|---------|------|----------|
| ì§§ì€ ê²°ê³¼ | <4000 | Direct | 0 |
| ì¤‘ê°„ ê²°ê³¼ | 4000-8000 | Direct | 0 |
| ê¸´ ê²°ê³¼ | 8000-20000 | Map-Reduce | 2-4 |
| ë§¤ìš° ê¸´ ê²°ê³¼ | 20000+ | Map-Reduce | 5+ |

---

## Consequences

### Positive

1. âœ… **í† í° ì œí•œ í•´ê²°**: ì–´ë–¤ ê¸¸ì´ì˜ ì…ë ¥ë„ ì²˜ë¦¬ ê°€ëŠ¥
2. âœ… **ì •ë³´ ë³´ì¡´**: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í™œìš©
3. âœ… **ë³‘ë ¬ ì²˜ë¦¬**: vLLM batch inferenceë¡œ Map ë‹¨ê³„ ë³‘ë ¬í™”
4. âœ… **ì ì‘ì **: ì§§ì€ ì…ë ¥ì€ ì¶”ê°€ í˜¸ì¶œ ì—†ì´ ì²˜ë¦¬

### Negative

1. âŒ **ì¶”ê°€ ì§€ì—°**: Map-Reduce ì‹œ 2ë‹¨ê³„ ì¶”ë¡  í•„ìš”
2. âŒ **ìš”ì•½ í’ˆì§ˆ ì˜ì¡´**: ì¤‘ê°„ ìš”ì•½ í’ˆì§ˆì´ ìµœì¢… ê²°ê³¼ ì˜í–¥

---

## Example Output

```
ğŸ“š Found 8 search results
ğŸ“Š Total Context Tokens: 15234 (Limit: 6294)
âš ï¸  Context exceeds limit. Triggering Map-Reduce...
ğŸ§© Split into 4 chunks for parallel summarization.
ğŸš€ Running batch inference for 4 chunks...
ğŸ”— Combining intermediate summaries...
ğŸ“‰ Reduced Context Tokens: 2841
ğŸ§  Analyzing with vLLM (Final Pass)...
âœ… Analysis completed in 3456ms (Total)
```

---

## References

- [LangChain Map-Reduce](https://python.langchain.com/docs/tutorials/summarization/#map-reduce)
- [vLLM Batch Inference](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
