# 10. Automatic GPU Model Selection

* Status: Accepted
* Date: 2026-01-14
* Context: Phase 3 (AI Worker Enhancement)

## Context and Problem Statement

AI WorkerÏóêÏÑú vLLM Î™®Îç∏ ÏÇ¨Ïö© Ïãú Îã§ÏùåÍ≥º Í∞ôÏùÄ Î¨∏Ï†úÍ∞Ä ÏûàÏóàÏäµÎãàÎã§:

1. **ÌïòÎìúÏΩîÎî©Îêú Î™®Îç∏**: ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Î™®Îç∏ ÏßÄÏ†ï ÌïÑÏàò
2. **GPU Ìò∏ÌôòÏÑ± Î¨∏Ï†ú**: 12GB GPUÏóê 70B Î™®Îç∏ Î°úÎìú ÏãúÎèÑ ‚Üí OOM
3. **Î∞∞Ìè¨ Î≥µÏû°ÏÑ±**: GPU ÏÇ¨ÏñëÎ≥Ñ Îã§Î•∏ ÏÑ§Ï†ï ÌïÑÏöî
4. **Í∞úÎ∞ú ÌôòÍ≤Ω Îã§ÏñëÏÑ±**: RTX 4070 (12GB), RTX 3090 (24GB) Îì± ÌòºÏû¨

**ÌïÑÏöîÏÇ¨Ìï≠**:
- GPU VRAM ÏûêÎèô Í∞êÏßÄ
- VRAMÏóê ÎßûÎäî ÏµúÏ†Å Î™®Îç∏ ÏûêÎèô ÏÑ†ÌÉù
- ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Ïò§Î≤ÑÎùºÏù¥Îìú Í∞ÄÎä•

## Decision Drivers

* **ÏÇ¨Ïö© Ìé∏ÏùòÏÑ±**: ÏÑ§Ï†ï ÏóÜÏù¥ ÏµúÏ†Å Î™®Îç∏ ÏÇ¨Ïö©
* **ÏïàÏ†ïÏÑ±**: OOM ÏóêÎü¨ Î∞©ÏßÄ
* **ÌôïÏû•ÏÑ±**: Îã§ÏñëÌïú GPU ÏßÄÏõê
* **Ïú†Ïó∞ÏÑ±**: ÏàòÎèô Ïò§Î≤ÑÎùºÏù¥Îìú Í∞ÄÎä•

## Decision Outcome

**GPU VRAM ÏûêÎèô Í∞êÏßÄ Î∞è Î™®Îç∏ ÏûêÎèô ÏÑ†ÌÉù** Î°úÏßÅÏùÑ Íµ¨ÌòÑÌñàÏäµÎãàÎã§.

---

## Implementation Details

### 1. GPU Memory Detection

```python
# src/common/ai_worker_utils.py
def get_gpu_memory_gb():
    """Detect GPU VRAM in GB using nvidia-smi or pynvml."""
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.total", "--format=csv,noheader,nounits"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            memory_mb = int(result.stdout.strip().split('\n')[0])
            return memory_mb / 1024  # Convert to GB
    except Exception:
        pass
    
    # Fallback: try pynvml
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        return info.total / (1024 ** 3)
    except Exception:
        pass
    
    return None
```

### 2. Model Selection by VRAM

```python
def select_model_by_vram(vram_gb):
    """Select appropriate model based on available GPU VRAM."""
    model_configs = [
        # (min_vram_gb, model_name, quantization, max_model_len)
        (20, "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4", "awq", 8192),
        (10, "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4", "awq", 8192),
        (6,  "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4", "awq", 4096),
        (0,  "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4", "awq", 2048),
    ]
    
    for min_vram, model, quant, max_len in model_configs:
        if vram_gb >= min_vram:
            return model, quant, max_len
    
    return model_configs[-1][1], model_configs[-1][2], model_configs[-1][3]
```

### 3. AI Worker Integration

```python
# src/ai_worker/main.py
env_model = os.getenv("VLLM_MODEL")

if env_model:
    # Use explicitly specified model
    MODEL_NAME = env_model
    QUANTIZATION = os.getenv("VLLM_QUANTIZATION", "awq")
    MAX_MODEL_LEN = int(os.getenv("VLLM_MAX_MODEL_LEN", "4096"))
else:
    # Auto-select model based on GPU VRAM
    gpu_memory = get_gpu_memory_gb()
    if gpu_memory:
        print(f"üéÆ Detected GPU VRAM: {gpu_memory:.1f} GB")
        MODEL_NAME, QUANTIZATION, MAX_MODEL_LEN = select_model_by_vram(gpu_memory)
    else:
        # Fallback to safe default
        MODEL_NAME = "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
        QUANTIZATION = "awq"
        MAX_MODEL_LEN = 4096
```

---

## Model Tiers

| GPU VRAM | ÏÑ†ÌÉù Î™®Îç∏ | Context Length |
|----------|----------|----------------|
| 20GB+ | Llama 3.1 70B AWQ | 8192 |
| 10-20GB | Llama 3.1 8B AWQ | 8192 |
| 6-10GB | Llama 3.1 8B AWQ | 4096 |
| <6GB | Llama 3.1 8B AWQ | 2048 |

---

## Consequences

### Positive

1. ‚úÖ **Zero-Config Î∞∞Ìè¨**: GPUÏóê ÎßûÎäî Î™®Îç∏ ÏûêÎèô ÏÑ†ÌÉù
2. ‚úÖ **OOM Î∞©ÏßÄ**: VRAMÏóê ÎßûÎäî context length ÏûêÎèô Ï°∞Ï†ï
3. ‚úÖ **Ïú†Ïó∞ÏÑ±**: `VLLM_MODEL` ÌôòÍ≤ΩÎ≥ÄÏàòÎ°ú Ïò§Î≤ÑÎùºÏù¥Îìú Í∞ÄÎä•
4. ‚úÖ **ÌÖåÏä§Ìä∏ Ïö©Ïù¥**: Ìï®Ïàò Î∂ÑÎ¶¨Î°ú Îã®ÏúÑ ÌÖåÏä§Ìä∏ Í∞ÄÎä•

### Negative

1. ‚ùå **nvidia-smi ÏùòÏ°¥**: GPU ÏóÜÎäî ÌôòÍ≤ΩÏóêÏÑú Ìè¥Î∞± ÌïÑÏöî
2. ‚ùå **Î©ÄÌã∞ GPU ÎØ∏ÏßÄÏõê**: Ï≤´ Î≤àÏß∏ GPUÎßå Í∞êÏßÄ

---

## Testing

```python
# tests/test_ai_worker.py
def test_12gb_selects_8b_long_context():
    model, quant, max_len = select_model_by_vram(12.0)
    assert "8B" in model
    assert max_len == 8192

def test_24gb_plus_selects_70b():
    model, quant, max_len = select_model_by_vram(24.0)
    assert "70B" in model
```

---

## References

- [nvidia-smi Documentation](https://developer.nvidia.com/nvidia-system-management-interface)
- [pynvml](https://pypi.org/project/pynvml/)
- [vLLM Memory Management](https://docs.vllm.ai/en/latest/)
